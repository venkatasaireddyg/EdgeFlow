#!/usr/bin/env python3
"""
EdgeFlow Inference Engine for raspberry_pi
Generated by EdgeFlow Deployment Packager
"""

import logging
import os
import time
from typing import Any, Dict, Optional

import numpy as np

# Device-specific imports
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    try:
        import tensorflow.lite as tflite
    except ImportError:
        raise ImportError("TensorFlow Lite runtime not available")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EdgeFlowInference:
    """Device-optimized inference engine for raspberry_pi."""

    def __init__(self, model_path: str = "mobilenet_v2_keras.h5"):
        """Initialize inference engine."""
        self.model_path = model_path
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.device_type = "raspberry_pi"

        # Device-specific configuration
        self.buffer_size = 32
        self.quantize_type = "int8"

        self._load_model()

    def _load_model(self):
        """Load TensorFlow Lite model."""
        try:
            self.interpreter = tflite.Interpreter(model_path=self.model_path)
            self.interpreter.allocate_tensors()

            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()

            logger.info(f"Model loaded successfully for {self.device_type}")
            logger.info(f"Input shape: {self.input_details[0]['shape']}")
            logger.info(f"Output shape: {self.output_details[0]['shape']}")

        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise

    def predict(self, input_data: np.ndarray) -> np.ndarray:
        """Run inference on input data."""
        if self.interpreter is None:
            raise RuntimeError("Model not loaded")

        # Set input tensor
        self.interpreter.set_tensor(self.input_details[0]["index"], input_data)

        # Run inference
        start_time = time.perf_counter()
        self.interpreter.invoke()
        inference_time = time.perf_counter() - start_time

        # Get output
        output_data = self.interpreter.get_tensor(self.output_details[0]["index"])

        logger.debug(f"Inference time: {inference_time * 1000:.2f}ms")

        return output_data

    def benchmark(self, num_runs: int = 100) -> Dict[str, float]:
        """Benchmark inference performance."""
        if self.input_details is None:
            raise RuntimeError("Model not loaded")

        # Generate test input
        input_shape = self.input_details[0]["shape"]
        dtype = self.input_details[0]["dtype"]

        if dtype == np.float32:
            test_input = np.random.random(input_shape).astype(np.float32)
        elif dtype == np.int8:
            test_input = np.random.randint(-128, 127, size=input_shape, dtype=np.int8)
        else:
            test_input = np.random.random(input_shape).astype(np.float32)

        # Warmup
        for _ in range(10):
            self.predict(test_input)

        # Benchmark
        times = []
        for _ in range(num_runs):
            start_time = time.perf_counter()
            self.predict(test_input)
            times.append(time.perf_counter() - start_time)

        return {
            "mean_time_ms": np.mean(times) * 1000,
            "std_time_ms": np.std(times) * 1000,
            "min_time_ms": np.min(times) * 1000,
            "max_time_ms": np.max(times) * 1000,
            "throughput_fps": 1.0 / np.mean(times),
        }


def main():
    """Main function for standalone execution."""
    import argparse

    parser = argparse.ArgumentParser(description="EdgeFlow Inference")
    parser.add_argument("--model", default="mobilenet_v2_keras.h5", help="Model path")
    parser.add_argument("--input", help="Input data path")
    parser.add_argument("--benchmark", action="store_true", help="Run benchmark")
    parser.add_argument(
        "--runs", type=int, default=100, help="Number of benchmark runs"
    )

    args = parser.parse_args()

    # Initialize inference engine
    inference = EdgeFlowInference(args.model)

    if args.benchmark:
        print("Running benchmark...")
        results = inference.benchmark(args.runs)
        print(f"Mean inference time: {results['mean_time_ms']:.2f}ms")
        print(f"Throughput: {results['throughput_fps']:.2f} FPS")
    else:
        print("EdgeFlow inference engine ready")
        print(f"Device: raspberry_pi")
        print(f"Model: {args.model}")


if __name__ == "__main__":
    main()
